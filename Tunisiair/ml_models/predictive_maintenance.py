# -*- coding: utf-8 -*-
"""predictive maintenance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i31biu-Te0JfymFuLwgFgNbzaE_D8l95
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import keras


def train_model(model):
    import numpy as np

    np.random.seed(1234)
    PYTHONHASHSEED = 0

    target_var = ['Target_Remaining_Useful_Life']
    index_columns_names =  ["UnitNumber","Cycle"]
    op_settings_columns = ["Op_Setting_"+str(i) for i in range(1,4)]
    sensor_columns =["Sensor_"+str(i) for i in range(1,22)]
    column_names = index_columns_names + op_settings_columns + sensor_columns
    print(column_names)

    column_names_ = column_names + ["_", "__"]
    # load data
    train= pd.read_csv('ml_models/train_FD001.txt', sep=" ", header=None, names = column_names_)
    test = pd.read_csv('ml_models/test_FD001.txt', sep=" ", header=None, names = column_names_)
    print("train shape: ", train.shape, "test shape: ", test.shape)
    # drop pesky NULL columns
    train.drop(train.columns[[26, 27]], axis=1, inplace=True)
    test.drop(test.columns[[26, 27]], axis=1, inplace=True)
    train[train['UnitNumber'] == 1].head(5)
    test[test['UnitNumber'] == 1].head(5)

    max_cycle = train.groupby('UnitNumber')['Cycle'].max().reset_index()
    max_cycle.columns = ['UnitNumber', 'MaxOfCycle']
    train_merged = train.merge(max_cycle, left_on='UnitNumber', right_on='UnitNumber', how='inner')
    Target_Remaining_Useful_Life = train_merged["MaxOfCycle"] - train_merged["Cycle"]
    train_with_target = train_merged["Target_Remaining_Useful_Life"] = Target_Remaining_Useful_Life

    train_with_target = train_merged.drop("MaxOfCycle", axis=1)
    train_with_target[train_with_target['UnitNumber'] == 1].head(5)


    print("train_with_target.shape:", train_with_target.shape)
    leakage_to_drop = ['UnitNumber', 'Cycle', 'Op_Setting_1', 'Op_Setting_2', 'Op_Setting_3']  
    train_no_leakage = train_with_target.drop(leakage_to_drop, axis = 1)
    print("train_no_leakage.shape: ", train_no_leakage.shape)
    # set up features and target variable 
    y = train_no_leakage['Target_Remaining_Useful_Life']
    X = train_no_leakage.drop(['Target_Remaining_Useful_Life'], axis = 1)

    # I like to use a simple random forest to determine some of the most important/meaningful features. Can be used as feature selection
    # create an exhuastive random forest (200 trees up to 15 levels deep)
    from sklearn import ensemble
    "estimating the feature importance..."
    rf = ensemble.RandomForestRegressor()
    single_rf = ensemble.RandomForestRegressor(n_estimators = 200, max_depth = 15)
    single_rf.fit(X, y)
    y_pred = single_rf.predict(X)
    print("feature importance estimation completed")

    # graph feature importance
    import matplotlib.pyplot as plt
    importances = single_rf.feature_importances_
    indices = np.argsort(importances)[::-1]
    feature_names = X.columns    


    # list feature importance
    important_features = pd.Series(data=single_rf.feature_importances_,index=X.columns)
    important_features.sort_values(ascending=False,inplace=True)

    # based on the graphs as well as random forest feature importance, I will exclude sensors without much valuable information
    print(train_no_leakage.shape)
    vars_to_drop = ["Sensor_"+str(i) for i in [5, 15, 9, 17, 4, 18]]
    train_final = train_no_leakage.drop(vars_to_drop, axis = 1)
    print(train_final.shape)

    print("preprocessing...")
    # identify categorical and numeric fields
    from sklearn import preprocessing
    categorical = train_final.select_dtypes(include=['object'])
    numeric = train_final.select_dtypes(exclude=['object'])
    print(categorical.columns.values)
    # create dummy variables (if any categorical fields)
    for name, values in categorical.items():
        print(name)
        dummies = pd.get_dummies(values.str.strip(), prefix = name, dummy_na=True)
        numeric = pd.concat([numeric, dummies], axis=1)
    # imputation (if any NULL values)
    for name in numeric:
        print(name)
        if pd.isnull(numeric[name]).sum() > 0:
            numeric["%s_mi" % (name)] = pd.isnull(numeric[name])
            median = numeric[name].median()
            numeric[name] = numeric[name].apply(lambda x: median if pd.isnull(x) else x)
    y = numeric['Target_Remaining_Useful_Life']
    X = numeric.drop(['Target_Remaining_Useful_Life'], axis = 1)

    # create holdout
    import numpy as np
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)
    # choose the model
    from sklearn.ensemble import GradientBoostingRegressor
    gb = ensemble.GradientBoostingRegressor()
    # set up 5-fold cross-validation
    from sklearn import model_selection
    cv = model_selection.KFold(5)
    # pipeline standardization and model
    from sklearn.pipeline import Pipeline
    pipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())
                               , ('model', gb) ])
    # tune the model
    my_alpha = [.5, .75, .9]
    my_n_estimators= [500]
    my_learning_rate = [0.005, .01]
    my_max_depth = [4, 5, 6]
    # run the model using gridsearch, select the model with best search
    from sklearn.model_selection import GridSearchCV
    print("training model...")
    optimized_gb = GridSearchCV(estimator=pipeline
                                , cv=cv
                                , param_grid =dict(model__max_depth = my_max_depth, model__n_estimators = my_n_estimators,
                                                  model__learning_rate = my_learning_rate, model__alpha = my_alpha)
                                , scoring = 'neg_mean_squared_error'
                                , verbose = 1
                                , n_jobs = -1
                               )
    optimized_gb.fit(X_train, y_train)
    # show the best model estimators
    print(optimized_gb.best_estimator_)
    # evaluate metrics on holdout
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    y_pred = optimized_gb.predict(X_test)
    print("Gradient Boosting Mean Squared Error: ", mean_squared_error(y_test, y_pred))
    print("Gradient Boosting Mean Absolute Error: ", mean_absolute_error(y_test, y_pred))
    print("Gradient Boosting r-squared: ", r2_score(y_test, y_pred))
    model["ready"]=True
    model["model"]=optimized_gb
    print("training completed !")



"""from joblib import dump
dump(optimized_gb, "predictive_maintenance.joblib")


from joblib import load
model = load('predictive_maintenance.joblib')

test = pd.read_csv('test_FD001.txt', sep=" ", header=None, names = column_names_)
vars_to_drop = ["Sensor_"+str(i) for i in [5, 15, 9, 17, 4, 18]]
vars_to_drop = vars_to_drop + ["_", "__", "UnitNumber", "Cycle", "Op_Setting_1", "Op_Setting_2", "Op_Setting_3"]
test = test.drop(vars_to_drop, axis = 1)
prediction = model.predict(test)
y = open("RUL_FD001.txt","r").read().split("\n")[:-1]
y = [int(i.replace(" ", "")) for i in y]

plt.scatter([i for i in range(len(y))], y, marker = ".")
plt.scatter([i for i in range(len(prediction))], prediction, c = "red", marker = ".")
plt.show()

prediction = [str(i) for i in prediction]

open("res.txt", "w").write("\n".join(prediction))"""